{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Leer y procesar imágenes para entrenar:"
      ],
      "metadata": {
        "id": "XgbDAoBSbthe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OGdE2WR3ADH",
        "outputId": "1815d1b8-cc3b-4bc9-f704-5d3b0e597787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib as plt\n",
        "from keras import layers, models\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# tamaño de imagen y epocas del entrenamiento\n",
        "width = 100\n",
        "height = 100\n",
        "epochs = 20\n",
        "\n",
        "ruta_train = '/content/drive/My Drive/chest_xray/train/'\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "# leer y procesar imagenes\n",
        "for i in os.listdir(ruta_train):\n",
        "    for j in os.listdir(ruta_train + i):\n",
        "        img = cv2.imread(ruta_train + i + '/' + j, cv2.IMREAD_GRAYSCALE)  # leer imagen en escala de grises\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        resized = cv2.resize(img, (width, height))\n",
        "        resized = resized / 255.0  # normalizar a [0, 1]\n",
        "\n",
        "        train_x.append(resized)\n",
        "\n",
        "        if i == \"NORMAL\":\n",
        "            train_y.append([0, 1])\n",
        "        else:\n",
        "            train_y.append([1, 0])\n",
        "\n",
        "x_data = np.array(train_x)\n",
        "y_data = np.array(train_y)\n",
        "\n",
        "# añadir una dimensión extra para que coincida con la entrada del modelo (capa Conv2D espera (width, height, 1))\n",
        "x_data = np.expand_dims(x_data, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición y entrenamiento del modelo:"
      ],
      "metadata": {
        "id": "hBWQ_Fmtbz29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(1000, (3, 3), input_shape=(width, height, 1), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    # layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_data, y_data, epochs=epochs)\n",
        "\n",
        "model.save('aprendePorfavor.keras')"
      ],
      "metadata": {
        "id": "wDZhozGabYv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fdb2bd7-9092-483b-ce70-250f4374e843"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "84/84 [==============================] - 113s 1s/step - loss: 0.5609 - accuracy: 0.6928\n",
            "Epoch 2/20\n",
            "84/84 [==============================] - 114s 1s/step - loss: 0.3280 - accuracy: 0.8635\n",
            "Epoch 3/20\n",
            "84/84 [==============================] - 114s 1s/step - loss: 0.2533 - accuracy: 0.8982\n",
            "Epoch 4/20\n",
            "84/84 [==============================] - 114s 1s/step - loss: 0.2270 - accuracy: 0.9165\n",
            "Epoch 5/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.2554 - accuracy: 0.9031\n",
            "Epoch 6/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.2449 - accuracy: 0.9016\n",
            "Epoch 7/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1862 - accuracy: 0.9217\n",
            "Epoch 8/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.2147 - accuracy: 0.9060\n",
            "Epoch 9/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1651 - accuracy: 0.9396\n",
            "Epoch 10/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1572 - accuracy: 0.9366\n",
            "Epoch 11/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1487 - accuracy: 0.9430\n",
            "Epoch 12/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1390 - accuracy: 0.9463\n",
            "Epoch 13/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1304 - accuracy: 0.9467\n",
            "Epoch 14/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1299 - accuracy: 0.9444\n",
            "Epoch 15/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1147 - accuracy: 0.9530\n",
            "Epoch 16/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1188 - accuracy: 0.9500\n",
            "Epoch 17/20\n",
            "84/84 [==============================] - 115s 1s/step - loss: 0.1328 - accuracy: 0.9519\n",
            "Epoch 18/20\n",
            "84/84 [==============================] - 116s 1s/step - loss: 0.1026 - accuracy: 0.9564\n",
            "Epoch 19/20\n",
            "84/84 [==============================] - 116s 1s/step - loss: 0.1227 - accuracy: 0.9497\n",
            "Epoch 20/20\n",
            "84/84 [==============================] - 116s 1s/step - loss: 0.0981 - accuracy: 0.9620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar y procesar imágenes para evaluar el modelo:"
      ],
      "metadata": {
        "id": "6mvLjPHbb5S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, models\n",
        "from google.colab import drive\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "#tamaño de imagen\n",
        "width = 100\n",
        "height = 100\n",
        "\n",
        "# ruta hacia las imagenes para testear\n",
        "ruta_test = '/content/drive/My Drive/chest_xray/test/'\n",
        "\n",
        "test_x = []\n",
        "test_y = []\n",
        "\n",
        "# cargar el modelo\n",
        "model = models.load_model('aprendePorfavor.keras')\n",
        "\n",
        "# leer y procesar imagenes\n",
        "for i in os.listdir(ruta_test):\n",
        "    for j in os.listdir(ruta_test + i):\n",
        "        img = cv2.imread(ruta_test + i + '/' + j, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        resized = cv2.resize(img, (width, height))\n",
        "        resized = resized / 255.0\n",
        "\n",
        "        test_x.append(resized)\n",
        "\n",
        "        if i == \"NORMAL\":\n",
        "            test_y.append([0, 1])\n",
        "        else:\n",
        "            test_y.append([1, 0])\n",
        "\n",
        "x_test = np.array(test_x)\n",
        "y_test = np.array(test_y)\n",
        "\n",
        "x_test = np.expand_dims(x_test, axis=-1)"
      ],
      "metadata": {
        "id": "HoZHb5pr4RUF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar el modelo:"
      ],
      "metadata": {
        "id": "cv5xTAaTcZGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "XQUl1uuPcWvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e982fd71-c6d5-4954-d75b-770244906101"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 7s 350ms/step - loss: 0.8880 - accuracy: 0.7917\n",
            "Test accuracy: 0.7916666865348816\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}