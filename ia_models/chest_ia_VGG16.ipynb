{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlw5hn2UGRYn",
        "outputId": "600df546-cd24-4cec-ae7b-26e1a58acc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "67/67 [==============================] - 54s 786ms/step - loss: 0.7885 - accuracy: 0.8235 - val_loss: 0.2008 - val_accuracy: 0.9255\n",
            "Epoch 2/10\n",
            "67/67 [==============================] - 52s 770ms/step - loss: 0.2166 - accuracy: 0.9191 - val_loss: 0.1719 - val_accuracy: 0.9348\n",
            "Epoch 3/10\n",
            "67/67 [==============================] - 51s 766ms/step - loss: 0.1946 - accuracy: 0.9295 - val_loss: 0.1598 - val_accuracy: 0.9330\n",
            "Epoch 4/10\n",
            "67/67 [==============================] - 52s 773ms/step - loss: 0.2156 - accuracy: 0.9262 - val_loss: 0.1826 - val_accuracy: 0.9255\n",
            "Epoch 5/10\n",
            "67/67 [==============================] - 52s 768ms/step - loss: 0.1831 - accuracy: 0.9423 - val_loss: 0.1130 - val_accuracy: 0.9590\n",
            "Epoch 6/10\n",
            "67/67 [==============================] - 51s 760ms/step - loss: 0.1967 - accuracy: 0.9300 - val_loss: 0.1543 - val_accuracy: 0.9348\n",
            "Epoch 7/10\n",
            "67/67 [==============================] - 51s 763ms/step - loss: 0.1822 - accuracy: 0.9352 - val_loss: 0.1292 - val_accuracy: 0.9441\n",
            "Epoch 8/10\n",
            "67/67 [==============================] - 51s 760ms/step - loss: 0.1711 - accuracy: 0.9404 - val_loss: 0.1579 - val_accuracy: 0.9665\n",
            "Epoch 9/10\n",
            "67/67 [==============================] - 51s 777ms/step - loss: 0.2164 - accuracy: 0.9177 - val_loss: 0.1065 - val_accuracy: 0.9609\n",
            "Epoch 10/10\n",
            "67/67 [==============================] - 51s 754ms/step - loss: 0.1773 - accuracy: 0.9380 - val_loss: 0.1543 - val_accuracy: 0.9404\n",
            "17/17 - 10s - loss: 0.1543 - accuracy: 0.9404 - 10s/epoch - 588ms/step\n",
            "Validation accuracy: 0.9404096603393555\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 301s 4s/step - loss: 0.1865 - accuracy: 0.9337 - val_loss: 0.0959 - val_accuracy: 0.9683\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 301s 4s/step - loss: 0.1488 - accuracy: 0.9590 - val_loss: 0.1049 - val_accuracy: 0.9609\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 302s 5s/step - loss: 0.1339 - accuracy: 0.9580 - val_loss: 0.2438 - val_accuracy: 0.9088\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 298s 4s/step - loss: 0.1180 - accuracy: 0.9673 - val_loss: 0.2328 - val_accuracy: 0.9143\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 296s 4s/step - loss: 0.1174 - accuracy: 0.9659 - val_loss: 0.1378 - val_accuracy: 0.9479\n",
            "17/17 - 11s - loss: 0.1378 - accuracy: 0.9479 - 11s/epoch - 628ms/step\n",
            "Validation accuracy after fine-tuning: 0.947858452796936\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "width = 224\n",
        "height = 224\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "ruta_train = '/content/drive/My Drive/chest_xray/train/'\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "# Leemos y procesamos imágenes\n",
        "for i in os.listdir(ruta_train):\n",
        "    for j in os.listdir(ruta_train + i):\n",
        "        img = cv2.imread(ruta_train + i + '/' + j, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        resized = cv2.resize(img, (width, height))\n",
        "        resized = resized / 255.0 # Normalizacion\n",
        "\n",
        "        # Convertimos imagen de 1 canal (escala de grises) a 3 canales para VGG16\n",
        "        resized = np.stack((resized,)*3, axis=-1)\n",
        "\n",
        "        train_x.append(resized)\n",
        "\n",
        "        if i == \"NORMAL\":\n",
        "            train_y.append([0, 1])\n",
        "        else:\n",
        "            train_y.append([1, 0])\n",
        "\n",
        "x_data = np.array(train_x)\n",
        "y_data = np.array(train_y)\n",
        "\n",
        "# Dividimos en conjunto de entrenamiento y validación\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Usamos la arquitectura VGG16 preentrenada\n",
        "def create_model():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(height, width, 3))\n",
        "\n",
        "    # \"Deshabilitamos\" algunas capas para solo entrenar las capas personalizadas\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Capas personalizadas\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model, base_model\n",
        "\n",
        "model, base_model = create_model()\n",
        "\n",
        "# aumentación de datos\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "# Entrenamiento\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluar el modelo entrenado\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val, verbose=2)\n",
        "print(f\"Validation accuracy: {val_acc}\")\n",
        "\n",
        "# \"Habilitamos\" las capaz previamente deshabilitadas para fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-5), metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos de nuevo con todas las capaz\n",
        "history_finetune = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(x_train) // batch_size,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluar después del fine-tuning\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val, verbose=2)\n",
        "print(f\"Validation accuracy after fine-tuning: {val_acc}\")\n",
        "\n",
        "model.save('modelo_vgg16_finetuned.keras')\n"
      ]
    }
  ]
}